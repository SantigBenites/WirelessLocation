{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8af275",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Data format:\n",
    "\n",
    "| location_x | location_y | AP1_rssi | AP2_rssi | AP3_rssi |\n",
    "|------------|------------|----------|----------|----------|\n",
    "|relative x location|relative y location|AP1 RSSI|AP2 RSSI|AP3 RSSI|\n",
    "\n",
    "\n",
    "We will have a cmongodb collectrion per geometric form we are using for training\n",
    "\n",
    "The training will be done in 2 fases, one where we combine all the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86ba195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "    \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess the MongoDB documents into a single array with 5 columns.\n",
    "    Columns: AP1_rssi, AP2_rssi, AP3_rssi, location_x, location_y\n",
    "    \n",
    "    Handles NaN values by:\n",
    "    1. Replacing NaN RSSI values with -100 (standard for missing signal)\n",
    "    2. Ensuring coordinates are always valid numbers\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Safely extract RSSI values, handling missing/NaN values\n",
    "        rssi_values = [\n",
    "            float(entry.get('AP1_rssi', -100)) if entry.get('AP1_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP2_rssi', -100)) if entry.get('AP2_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP3_rssi', -100)) if entry.get('AP3_rssi', -100) != None else -100\n",
    "        ]\n",
    "        \n",
    "        # Validate coordinates\n",
    "        try:\n",
    "            x_coord = float(entry['location_x'])\n",
    "            y_coord = float(entry['location_y'])\n",
    "            if np.isnan(x_coord) or np.isnan(y_coord):\n",
    "                continue  # Skip this entry if coordinates are invalid\n",
    "        except (KeyError, ValueError):\n",
    "            continue  # Skip this entry if coordinates are missing or invalid\n",
    "            \n",
    "        # Combine all values into one row\n",
    "        combined_row = rssi_values + [x_coord, y_coord]\n",
    "        combined_data.append(combined_row)\n",
    "    \n",
    "    # Convert to numpy array and verify no NaNs remain\n",
    "    result = np.array(combined_data, dtype=np.float32)\n",
    "    assert not np.isnan(result).any(), \"NaN values detected in final output!\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_dataset(collection_name, db_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        collection_name (str): Name of the MongoDB collection to use\n",
    "        db_name (str): Name of the MongoDB database\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('mongodb://localhost:28910/')\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Load all data from the collection\n",
    "    data = list(collection.find())\n",
    "    \n",
    "    # Preprocess the data to extract features and labels\n",
    "    return process_data(data)\n",
    "\n",
    "\n",
    "def split_combined_data(combined_array, num_ap=3):\n",
    "\n",
    "    # Split the array into features (RSSI values) and labels (coordinates)\n",
    "    features = combined_array[:, :num_ap]  # First num_ap columns are RSSI values\n",
    "    labels = combined_array[:, num_ap:]    # Last 2 columns are coordinates\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def combine_arrays(arrays):\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "def shuffle_array(arr, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    shuffled_arr = arr.copy()\n",
    "    np.random.shuffle(shuffled_arr)\n",
    "    return shuffled_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d3eca",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WiFiPositionModel(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=2):\n",
    "        super(WiFiPositionModel, self).__init__()\n",
    "        \n",
    "        # Feature extraction branch\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Position prediction branch\n",
    "        self.position_net = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "        \n",
    "        # Uncertainty estimation\n",
    "        self.uncertainty_net = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.feature_net(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(features)\n",
    "        attended_features = features * attention_weights\n",
    "        \n",
    "        # Position prediction\n",
    "        position = self.position_net(attended_features)\n",
    "        \n",
    "        # Uncertainty estimation\n",
    "        uncertainty = self.uncertainty_net(attended_features)\n",
    "        \n",
    "        return position, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_wifi_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train a neural network model for WiFi positioning and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.array): Training features (RSSI values)\n",
    "        y_train (np.array): Training labels (coordinates)\n",
    "        X_val (np.array): Validation features\n",
    "        y_val (np.array): Validation labels\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained model, training history dictionary)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    print(f\"Training tensor is on: {X_train_tensor.device}\")\n",
    "    \n",
    "    # Create DataLoader for batch training\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = WiFiPositionModel(input_size=X_train.shape[1]).to(device)\n",
    "    print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)  # Only use position output\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        # Calculate epoch training loss\n",
    "        train_loss = np.mean(batch_losses)\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs, _ = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "            val_loss_history.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training set evaluation\n",
    "        train_preds, _ = model(X_train_tensor)\n",
    "        train_preds = train_preds.cpu().numpy()  # Move back to CPU for sklearn\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "        train_mae = mean_absolute_error(y_train, train_preds)\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_preds, _ = model(X_val_tensor)\n",
    "        val_preds = val_preds.cpu().numpy()\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        val_mae = mean_absolute_error(y_val, val_preds)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot actual vs predicted for validation set\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_val[:, 0], y_val[:, 1], label='Actual', alpha=0.5)\n",
    "    plt.scatter(val_preds[:, 0], val_preds[:, 1], label='Predicted', alpha=0.5)\n",
    "    plt.title('Actual vs Predicted Positions')\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(f\"Training RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Return model and history\n",
    "    history = {\n",
    "        'train_loss': train_loss_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'train_metrics': {'rmse': train_rmse, 'mae': train_mae},\n",
    "        'val_metrics': {'rmse': val_rmse, 'mae': val_mae}\n",
    "    }\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e923e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'model' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m X_train, X_val, y_train, y_val = train_test_split(global_array_x, global_array_y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#X_train, X_val, y_train, y_val = training_x, validation_x, training_y, validation_y\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrain_wifi_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_wifi_model\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mTrain a neural network model for WiFi positioning and visualize results.\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[33;03m    tuple: (trained model, training history dictionary)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[43mmodel\u001b[49m.parameters()).device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining tensor is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_tensor.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Convert numpy arrays to PyTorch tensors\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'model' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Get datasets from all collections\n",
    "datasets = [\n",
    "    get_dataset(\"wifi_data_reto_grande\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_pequeno\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\")\n",
    "]\n",
    "\n",
    "# Combine all datasets into one array\n",
    "combined_data = combine_arrays(datasets)\n",
    "\n",
    "# Shuffle the combined data\n",
    "shuffled_data = shuffle_array(combined_data)\n",
    "\n",
    "# Split into features and labels\n",
    "#training_x, training_y = split_combined_data(shuffled_data)\n",
    "#validation_x, validation_y = split_combined_data(get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\"))\n",
    "global_array_x, global_array_y = split_combined_data(shuffled_data)\n",
    "\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(global_array_x, global_array_y, test_size=0.2, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = training_x, validation_x, training_y, validation_y\n",
    "\n",
    "train_wifi_model(X_train, y_train, X_val, y_val,epochs=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "location-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
