{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8af275",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Data format:\n",
    "\n",
    "| location_x | location_y | AP1_rssi | AP2_rssi | AP3_rssi |\n",
    "|------------|------------|----------|----------|----------|\n",
    "|relative x location|relative y location|AP1 RSSI|AP2 RSSI|AP3 RSSI|\n",
    "\n",
    "\n",
    "We will have a cmongodb collectrion per geometric form we are using for training\n",
    "\n",
    "The training will be done in 2 fases, one where we combine all the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86ba195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "\n",
    "    \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess the MongoDB documents into a single array with 5 columns.\n",
    "    Columns: AP1_rssi, AP2_rssi, AP3_rssi, location_x, location_y\n",
    "    \n",
    "    Handles NaN values by:\n",
    "    1. Replacing NaN RSSI values with -100 (standard for missing signal)\n",
    "    2. Ensuring coordinates are always valid numbers\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Safely extract RSSI values, handling missing/NaN values\n",
    "        rssi_values = [\n",
    "            float(entry.get('AP1_rssi', -100)) if entry.get('AP1_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP2_rssi', -100)) if entry.get('AP2_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP3_rssi', -100)) if entry.get('AP3_rssi', -100) != None else -100\n",
    "        ]\n",
    "        \n",
    "        # Validate coordinates\n",
    "        try:\n",
    "            x_coord = float(entry['location_x'])\n",
    "            y_coord = float(entry['location_y'])\n",
    "            if np.isnan(x_coord) or np.isnan(y_coord):\n",
    "                continue  # Skip this entry if coordinates are invalid\n",
    "        except (KeyError, ValueError):\n",
    "            continue  # Skip this entry if coordinates are missing or invalid\n",
    "            \n",
    "        # Combine all values into one row\n",
    "        combined_row = rssi_values + [x_coord, y_coord]\n",
    "        combined_data.append(combined_row)\n",
    "    \n",
    "    # Convert to numpy array and verify no NaNs remain\n",
    "    result = np.array(combined_data, dtype=np.float32)\n",
    "    assert not np.isnan(result).any(), \"NaN values detected in final output!\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_dataset(collection_name, db_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        collection_name (str): Name of the MongoDB collection to use\n",
    "        db_name (str): Name of the MongoDB database\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('mongodb://localhost:28910/')\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Load all data from the collection\n",
    "    data = list(collection.find())\n",
    "    \n",
    "    # Preprocess the data to extract features and labels\n",
    "    return process_data(data)\n",
    "\n",
    "\n",
    "def split_combined_data(combined_array, num_ap=3):\n",
    "\n",
    "    # Split the array into features (RSSI values) and labels (coordinates)\n",
    "    features = combined_array[:, :num_ap]  # First num_ap columns are RSSI values\n",
    "    labels = combined_array[:, num_ap:]    # Last 2 columns are coordinates\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def combine_arrays(arrays):\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "def shuffle_array(arr, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    shuffled_arr = arr.copy()\n",
    "    np.random.shuffle(shuffled_arr)\n",
    "    return shuffled_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d3eca",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe4b6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WiFiPositionModel(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=2):\n",
    "        super(WiFiPositionModel, self).__init__()\n",
    "        \n",
    "        # Feature extraction branch\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Position prediction branch\n",
    "        self.position_net = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "        \n",
    "        # Uncertainty estimation\n",
    "        self.uncertainty_net = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.feature_net(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(features)\n",
    "        attended_features = features * attention_weights\n",
    "        \n",
    "        # Position prediction\n",
    "        position = self.position_net(attended_features)\n",
    "        \n",
    "        # Uncertainty estimation\n",
    "        uncertainty = self.uncertainty_net(attended_features)\n",
    "        \n",
    "        return position, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0afb89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def train_wifi_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train a neural network model for WiFi positioning and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.array): Training features (RSSI values)\n",
    "        y_train (np.array): Training labels (coordinates)\n",
    "        X_val (np.array): Validation features\n",
    "        y_val (np.array): Validation labels\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained model, training history dictionary)\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val)\n",
    "    \n",
    "    # Create DataLoader for batch training\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = WiFiPositionModel(input_size=X_train.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)  # Only use position output\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        # Calculate epoch training loss\n",
    "        train_loss = np.mean(batch_losses)\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs, _ = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "            val_loss_history.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training set evaluation\n",
    "        train_preds, _ = model(X_train_tensor)\n",
    "        train_preds = train_preds.numpy()\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "        train_mae = mean_absolute_error(y_train, train_preds)\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        val_preds, _ = model(X_val_tensor)\n",
    "        val_preds = val_preds.numpy()\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        val_mae = mean_absolute_error(y_val, val_preds)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot actual vs predicted for validation set\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_val[:, 0], y_val[:, 1], label='Actual', alpha=0.5)\n",
    "    plt.scatter(val_preds[:, 0], val_preds[:, 1], label='Predicted', alpha=0.5)\n",
    "    plt.title('Actual vs Predicted Positions')\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(f\"Training RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Return model and history\n",
    "    history = {\n",
    "        'train_loss': train_loss_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'train_metrics': {'rmse': train_rmse, 'mae': train_mae},\n",
    "        'val_metrics': {'rmse': val_rmse, 'mae': val_mae}\n",
    "    }\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e923e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Train Loss: 0.0365, Val Loss: 0.0318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(global_array_x, global_array_y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#X_train, X_val, y_train, y_val = training_x, validation_x, training_y, validation_y\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain_wifi_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 53\u001b[0m, in \u001b[0;36mtrain_wifi_model\u001b[0;34m(X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     52\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 53\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate epoch training loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/wireless_location_env/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/wireless_location_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/wireless_location_env/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/wireless_location_env/lib/python3.10/site-packages/torch/optim/_functional.py:98\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     97\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get datasets from all collections\n",
    "datasets = [\n",
    "    get_dataset(\"wifi_data_reto_grande\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_pequeno\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\")\n",
    "]\n",
    "\n",
    "# Combine all datasets into one array\n",
    "combined_data = combine_arrays(datasets)\n",
    "\n",
    "# Shuffle the combined data\n",
    "shuffled_data = shuffle_array(combined_data)\n",
    "\n",
    "# Split into features and labels\n",
    "#training_x, training_y = split_combined_data(shuffled_data)\n",
    "#validation_x, validation_y = split_combined_data(get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\"))\n",
    "global_array_x, global_array_y = split_combined_data(shuffled_data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(global_array_x, global_array_y, test_size=0.2, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = training_x, validation_x, training_y, validation_y\n",
    "\n",
    "train_wifi_model(X_train, y_train, X_val, y_val,epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless_location_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
