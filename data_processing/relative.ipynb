{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df639d86",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "In the expriment previsoly made we used 2 axis systems.\n",
    "\n",
    "One relative to the APs, which was 4x4\n",
    "One relative to the picos, which was 10x10\n",
    "\n",
    "To this effect our first step to relativize our points will be to create 2 functions:\n",
    "- One for the AP positions, which passes 4x4 to a normal value\n",
    "- One for the pico positions, which passes 10x10 to a normal value\n",
    "\n",
    "A normal value in our situation will consist of a value between -1,0 and 1\n",
    "The origin for out axis system will be one of the APs\n",
    "The maximum and minium of our normalization depend on the space between samples, therefore we will take our axis that goes from 0 to 1 and subdivide it into 10 segments, give we have 10 pico samples.\n",
    "The size of one 1 of the segments will be out unit in the final normalized space (both in lenght and width)\n",
    "With this in mind we will have 100 areas with 1 segment in length and width\n",
    "In each triangle configuration we will then move the origin to one of ther vertices and map out the datapoints corresponding to where the land in regards to the origin taking into account the coordinate system made from the segments/areas.\n",
    "These will be our normalized points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c28ccd",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ad70ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:28910/\")\n",
    "db = client[\"wifi_data_db\"]\n",
    "\n",
    "\n",
    "AP_BSSID = {\n",
    "    \"ec:01:d5:2b:5f:e0\": \"Freind1\",\n",
    "    \"ec:01:d5:27:1d:00\": \"Freind2\",\n",
    "    \"ec:01:d5:28:fa:c0\": \"Freind3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d768dd",
   "metadata": {},
   "source": [
    "# Join Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802db303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection wifi_client_data has 67340 documents\n",
      "Collection wifi_client_data_1 has 43360 documents\n",
      "Processed wifi_client_data (1/2)\n",
      "Processed wifi_client_data_1 (2/2)\n",
      "Output collection wifi_client_data_global has 110700 documents\n",
      "Expected total (sum of inputs): 110700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110700"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def union_collections_preserve_duplicates(collections_to_join, output_collection):\n",
    "    \n",
    "    # Verify input collections exist and get counts\n",
    "    input_counts = {}\n",
    "    for coll_name in collections_to_join:\n",
    "        if coll_name not in db.list_collection_names():\n",
    "            raise ValueError(f\"Collection {coll_name} does not exist\")\n",
    "        input_counts[coll_name] = db[coll_name].count_documents({})\n",
    "        print(f\"Collection {coll_name} has {input_counts[coll_name]} documents\")\n",
    "    \n",
    "    # Create a pipeline that adds source collection info and merges\n",
    "    pipelines = []\n",
    "    \n",
    "    # First pipeline creates the output collection with modified _id\n",
    "    first_coll = collections_to_join[0]\n",
    "    pipelines.append([\n",
    "        {\"$addFields\": {\n",
    "            \"original_id\": \"$_id\",\n",
    "            \"_id\": {\"$concat\": [first_coll, \"||\", {\"$toString\": \"$_id\"}]},\n",
    "            \"source_collection\": first_coll\n",
    "        }},\n",
    "        {\"$out\": output_collection}\n",
    "    ])\n",
    "    \n",
    "    # Subsequent pipelines merge with modified _id\n",
    "    for coll_name in collections_to_join[1:]:\n",
    "        pipelines.append([\n",
    "            {\"$addFields\": {\n",
    "                \"original_id\": \"$_id\",\n",
    "                \"_id\": {\"$concat\": [coll_name, \"||\", {\"$toString\": \"$_id\"}]},\n",
    "                \"source_collection\": coll_name\n",
    "            }},\n",
    "            {\"$merge\": {\n",
    "                \"into\": output_collection,\n",
    "                \"whenMatched\": \"fail\",  # Shouldn't happen with our new _id scheme\n",
    "                \"whenNotMatched\": \"insert\"\n",
    "            }}\n",
    "        ])\n",
    "    \n",
    "    # Execute all pipelines\n",
    "    for i, (coll_name, pipeline) in enumerate(zip(collections_to_join, pipelines)):\n",
    "        db[coll_name].aggregate(pipeline)\n",
    "        print(f\"Processed {coll_name} ({i+1}/{len(collections_to_join)})\")\n",
    "    \n",
    "    # Verify the output\n",
    "    output_count = db[output_collection].count_documents({})\n",
    "    expected_total = sum(input_counts.values())\n",
    "    print(f\"Output collection {output_collection} has {output_count} documents\")\n",
    "    print(f\"Expected total (sum of inputs): {expected_total}\")\n",
    "    \n",
    "    if output_count != expected_total:\n",
    "        print(f\"Warning: Output count doesn't match sum of input collections\")\n",
    "    \n",
    "    return output_count\n",
    "\n",
    "# Example usage\n",
    "union_collections_preserve_duplicates(\n",
    "    collections_to_join=[\"wifi_client_data\", \"wifi_client_data_1\"],\n",
    "    output_collection=\"wifi_client_data_global\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaa99930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_picos_coordinates(x, y, origin_x, origin_y):\n",
    "\n",
    " \n",
    "    # Normlized interval sizes\n",
    "    pico_interval = 1 / 10 \n",
    "    ap_interval = 1/4   \n",
    "\n",
    "    # Normalized locations\n",
    "    normalized_x = pico_interval * x\n",
    "    normalized_y = pico_interval * y\n",
    "    normalized_origin_x = ap_interval * origin_x\n",
    "    normalized_origin_y = ap_interval * origin_y\n",
    "    \n",
    "    \n",
    "    return (normalized_x-normalized_origin_x, normalized_y-normalized_origin_y)\n",
    "\n",
    "\n",
    "def calculate_centroid(point1, point2, point3):\n",
    "    cx = (point1[0] + point2[0] + point3[0]) / 3\n",
    "    cy = (point1[1] + point2[1] + point3[1]) / 3\n",
    "    return (cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247f556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_wifi_data(db, origin_x=None, origin_y=None, start_time=None, end_time=None, dry_run=False, output_collection_name=\"wifi_data_filtered\", input_collection_name=\"wifi_data\"):\n",
    "    \"\"\"\n",
    "    Transform wifi scan data into filtered format with normalized coordinates.\n",
    "    \n",
    "    Args:\n",
    "        db: MongoDB database object\n",
    "        origin_x: Origin x-coordinate for normalization\n",
    "        origin_y: Origin y-coordinate for normalization\n",
    "        start_time: datetime object for start of time range (inclusive)\n",
    "        end_time: datetime object for end of time range (inclusive)\n",
    "        dry_run: If True, only preview changes without writing to DB\n",
    "        output_collection_name: Name of the collection to write processed data into\n",
    "        input_collection_name: Name of the input collection containing raw data\n",
    "    \"\"\"\n",
    "    ap_mapping = {\n",
    "        \"ec:01:d5:2b:5f:e0\": \"AP1_rssi\",\n",
    "        \"ec:01:d5:27:1d:00\": \"AP2_rssi\",\n",
    "        \"ec:01:d5:28:fa:c0\": \"AP3_rssi\"\n",
    "    }\n",
    "    \n",
    "    ip_to_y = {\n",
    "        31: 1, 32: 2, 33: 3, 34: 4, 35: 5,\n",
    "        36: 6, 37: 7, 38: 8, 39: 9, 30: 10\n",
    "    }\n",
    "    \n",
    "    match_stage = {}\n",
    "    if start_time:\n",
    "        match_stage[\"timestamp\"] = {\"$gte\": start_time.timestamp()}\n",
    "    if end_time:\n",
    "        match_stage.setdefault(\"timestamp\", {})[\"$lte\"] = end_time.timestamp()\n",
    "    \n",
    "    collection = db[input_collection_name]\n",
    "\n",
    "    pipeline = [\n",
    "        {\"$match\": match_stage} if match_stage else {\"$match\": {}},\n",
    "        {\n",
    "            \"$addFields\": {\n",
    "                \"ip_ending\": {\n",
    "                    \"$toInt\": {\"$arrayElemAt\": [{\"$split\": [\"$metadata.pico_ip\", \".\"]}, 3]}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"raw_location_x\": \"$metadata.button_id\",\n",
    "                \"raw_location_y\": {\n",
    "                    \"$switch\": {\n",
    "                        \"branches\": [\n",
    "                            {\"case\": {\"$eq\": [\"$ip_ending\", val]}, \"then\": ip_to_y[val]}\n",
    "                            for val in ip_to_y\n",
    "                        ],\n",
    "                        \"default\": None\n",
    "                    }\n",
    "                },\n",
    "                \"data\": 1,\n",
    "                \"timestamp\": 1\n",
    "            }\n",
    "        },\n",
    "        {\"$match\": {\"raw_location_y\": {\"$ne\": None}}},\n",
    "        {\"$unwind\": \"$data\"},\n",
    "        {\"$match\": {\"data.BSSID\": {\"$in\": list(ap_mapping.keys())}}},\n",
    "        {\n",
    "            \"$group\": {\n",
    "                \"_id\": {\n",
    "                    \"raw_location_x\": \"$raw_location_x\",\n",
    "                    \"raw_location_y\": \"$raw_location_y\",\n",
    "                    \"timestamp\": \"$timestamp\"\n",
    "                },\n",
    "                **{\n",
    "                    field_name: {\n",
    "                        \"$max\": {\n",
    "                            \"$cond\": [\n",
    "                                {\"$eq\": [\"$data.BSSID\", bssid]},\n",
    "                                \"$data.RSSI\",\n",
    "                                None\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                    for bssid, field_name in ap_mapping.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    \n",
    "    normalized_results = []\n",
    "    for doc in results:\n",
    "        raw_x = doc[\"_id\"][\"raw_location_x\"]\n",
    "        raw_y = doc[\"_id\"][\"raw_location_y\"]\n",
    "        \n",
    "        norm_x, norm_y = normalize_picos_coordinates(\n",
    "            raw_x, raw_y,\n",
    "            origin_x if origin_x is not None else 0,\n",
    "            origin_y if origin_y is not None else 0\n",
    "        )\n",
    "        \n",
    "        if dry_run:\n",
    "            new_doc = {\n",
    "                \"raw_location_x\": raw_x,\n",
    "                \"raw_location_y\": raw_y,\n",
    "                \"location_x\": norm_x,\n",
    "                \"location_y\": norm_y,\n",
    "                \"timestamp\": doc[\"_id\"][\"timestamp\"],\n",
    "                **{field: doc.get(field) for field in ap_mapping.values()}\n",
    "            }\n",
    "        else:\n",
    "            new_doc = {\n",
    "                \"location_x\": norm_x,\n",
    "                \"location_y\": norm_y,\n",
    "                \"timestamp\": doc[\"_id\"][\"timestamp\"],\n",
    "                **{field: doc.get(field) for field in ap_mapping.values()}\n",
    "            }\n",
    "        normalized_results.append(new_doc)\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"Dry run: Would process {len(normalized_results)} documents\")\n",
    "        if normalized_results:\n",
    "            print(\"Sample documents:\")\n",
    "            doc = normalized_results[0]\n",
    "            print(f\"  Raw location: {doc.get('raw_location_x')},{doc.get('raw_location_y')}\")\n",
    "            print(f\"  Normalized location: {doc['location_x']:.4f},{doc['location_y']:.4f}\")\n",
    "            print(f\"  timestamp: {datetime.fromtimestamp(doc['timestamp'])}\")\n",
    "            for ap in ap_mapping.values():\n",
    "                print(f\"  {ap}: {doc.get(ap, 'N/A')}\")\n",
    "            print()\n",
    "        return normalized_results\n",
    "    \n",
    "    if normalized_results:\n",
    "        db[output_collection_name].delete_many({})\n",
    "        db[output_collection_name].insert_many(normalized_results)\n",
    "        print(f\"Successfully processed {len(normalized_results)} documents into {output_collection_name}\")\n",
    "        return normalized_results\n",
    "    else:\n",
    "        print(\"No documents matched the criteria\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e0dcffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 19393 documents into wifi_data_reto_grande\n",
      "Successfully processed 21140 documents into wifi_data_reto_medio\n",
      "Successfully processed 23533 documents into wifi_data_reto_pequeno\n",
      "Successfully processed 19318 documents into wifi_data_equilatero_grande\n",
      "Successfully processed 21845 documents into wifi_data_equilatero_medio\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "triangle_dictionary = {\n",
    "    \"reto_grande\": {\n",
    "        \"start\":datetime(2025, 5, 13, 20, 10),\n",
    "        \"end\":datetime(2025, 5, 13, 21, 42),\n",
    "        \"origin\":calculate_centroid((0,0),(4,0),(0,4))\n",
    "    },\n",
    "    \"reto_medio\": {\n",
    "        \"start\":datetime(2025, 5, 13, 21, 46),\n",
    "        \"end\":datetime(2025, 5, 13, 22, 49),\n",
    "        \"origin\":calculate_centroid((1,1),(3,1),(1,3))\n",
    "    },\n",
    "    \"reto_pequeno\": {\n",
    "        \"start\":datetime(2025, 5, 13, 22, 51),\n",
    "        \"end\":datetime(2025, 5, 13, 23, 53),\n",
    "        \"origin\":calculate_centroid((1,1),(2,1),(1,2))\n",
    "    },\n",
    "    \"equilatero_grande\": {\n",
    "        \"start\":datetime(2025, 6, 28, 19, 45),\n",
    "        \"end\":datetime(2025, 6, 28, 21, 15),\n",
    "        \"origin\":calculate_centroid((0,0),(4,0),(2,4))\n",
    "    },\n",
    "    \"equilatero_medio\": {\n",
    "        \"start\":datetime(2025, 6, 28, 22, 5),\n",
    "        \"end\":datetime(2025, 6, 28, 23, 30),\n",
    "        \"origin\":calculate_centroid((1,1),(2,1),(2,3))\n",
    "    },\n",
    "}\n",
    "collection = db[\"wifi_client_data\"]\n",
    "\n",
    "for triangle_name in triangle_dictionary.keys(): \n",
    "    input_collection = \"wifi_client_data_global\"\n",
    "    current_triangle = triangle_dictionary[triangle_name]\n",
    "    start_time  = current_triangle[\"start\"]\n",
    "    end_time    = current_triangle[\"end\"]\n",
    "    origin      = current_triangle[\"origin\"]\n",
    "\n",
    "    transform_wifi_data(db, origin[0], origin[1], start_time, end_time, dry_run=False, input_collection_name=input_collection, output_collection_name=f\"wifi_data_{triangle_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
