{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19daf641",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35b0c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "    \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess the MongoDB documents into a single array with 5 columns.\n",
    "    Columns: AP1_rssi, AP2_rssi, AP3_rssi, location_x, location_y\n",
    "    \n",
    "    Handles NaN values by:\n",
    "    1. Replacing NaN RSSI values with -100 (standard for missing signal)\n",
    "    2. Ensuring coordinates are always valid numbers\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Safely extract RSSI values, handling missing/NaN values\n",
    "        rssi_values = [\n",
    "            float(entry.get('AP1_rssi', -100)) if entry.get('AP1_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP2_rssi', -100)) if entry.get('AP2_rssi', -100) != None else -100,\n",
    "            float(entry.get('AP3_rssi', -100)) if entry.get('AP3_rssi', -100) != None else -100\n",
    "        ]\n",
    "        \n",
    "        # Validate coordinates\n",
    "        try:\n",
    "            x_coord = float(entry['location_x'])\n",
    "            y_coord = float(entry['location_y'])\n",
    "            if np.isnan(x_coord) or np.isnan(y_coord):\n",
    "                continue  # Skip this entry if coordinates are invalid\n",
    "        except (KeyError, ValueError):\n",
    "            continue  # Skip this entry if coordinates are missing or invalid\n",
    "            \n",
    "        # Combine all values into one row\n",
    "        combined_row = rssi_values + [x_coord, y_coord]\n",
    "        combined_data.append(combined_row)\n",
    "    \n",
    "    # Convert to numpy array and verify no NaNs remain\n",
    "    result = np.array(combined_data, dtype=np.float32)\n",
    "    assert not np.isnan(result).any(), \"NaN values detected in final output!\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_dataset(collection_name, db_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        collection_name (str): Name of the MongoDB collection to use\n",
    "        db_name (str): Name of the MongoDB database\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('mongodb://localhost:28910/')\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Load all data from the collection\n",
    "    data = list(collection.find())\n",
    "    \n",
    "    # Preprocess the data to extract features and labels\n",
    "    return process_data(data)\n",
    "\n",
    "\n",
    "def split_combined_data(combined_array, num_ap=3):\n",
    "\n",
    "    # Split the array into features (RSSI values) and labels (coordinates)\n",
    "    features = combined_array[:, :num_ap]  # First num_ap columns are RSSI values\n",
    "    labels = combined_array[:, num_ap:]    # Last 2 columns are coordinates\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def combine_arrays(arrays):\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "def shuffle_array(arr, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    shuffled_arr = arr.copy()\n",
    "    np.random.shuffle(shuffled_arr)\n",
    "    return shuffled_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92433a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GeneratedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, architecture_config):\n",
    "        super(GeneratedModel, self).__init__()\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.architecture_config = architecture_config\n",
    "        \n",
    "        # Build the network dynamically\n",
    "        prev_size = input_size\n",
    "        layer_counter = 1\n",
    "        \n",
    "        for layer_spec in architecture_config['hidden_layers']:\n",
    "            # Add linear layer\n",
    "            layer_size = layer_spec['units']\n",
    "            self.layers[f'linear_{layer_counter}'] = nn.Linear(prev_size, layer_size)\n",
    "            prev_size = layer_size\n",
    "            \n",
    "            # Add batch norm if specified\n",
    "            if layer_spec.get('batch_norm', False):\n",
    "                self.layers[f'batchnorm_{layer_counter}'] = nn.BatchNorm1d(layer_size)\n",
    "            \n",
    "            # Add activation\n",
    "            activation = layer_spec.get('activation', 'relu')\n",
    "            if activation == 'relu':\n",
    "                self.layers[f'activation_{layer_counter}'] = nn.ReLU()\n",
    "            elif activation == 'leaky_relu':\n",
    "                self.layers[f'activation_{layer_counter}'] = nn.LeakyReLU(0.1)\n",
    "            \n",
    "            # Add dropout if specified\n",
    "            if 'dropout' in layer_spec:\n",
    "                self.layers[f'dropout_{layer_counter}'] = nn.Dropout(layer_spec['dropout'])\n",
    "            \n",
    "            layer_counter += 1\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "        # Convert ModuleDict to Sequential\n",
    "        self.net = nn.Sequential(self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        position = self.output_layer(features)\n",
    "        return position, None  # Returning None for uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ca88f",
   "metadata": {},
   "source": [
    "# Generate all models for a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22514b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_configs(search_space):\n",
    "    \"\"\"\n",
    "    Generate all possible model configurations from the search space\n",
    "    \"\"\"\n",
    "    # Convert search space dict to lists of options\n",
    "    keys, values = zip(*search_space.items())\n",
    "    \n",
    "    # Generate all combinations\n",
    "    configs = []\n",
    "    for combination in product(*values):\n",
    "        config = dict(zip(keys, combination))\n",
    "        \n",
    "        # Build hidden layers specification\n",
    "        hidden_layers = []\n",
    "        for i in range(config['num_layers']):\n",
    "            layer_spec = {\n",
    "                'units': config['layer_size'],\n",
    "                'batch_norm': config['batch_norm'],\n",
    "                'activation': config['activation'],\n",
    "                'dropout': config['dropout'] if config['use_dropout'] else None\n",
    "            }\n",
    "            hidden_layers.append(layer_spec)\n",
    "        \n",
    "        # Create final architecture config\n",
    "        architecture_config = {\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'attention': config['attention'],\n",
    "            'uncertainty_estimation': config['uncertainty_estimation']\n",
    "        }\n",
    "        \n",
    "        configs.append({\n",
    "            'name': f\"Model_{len(configs)+1}\",\n",
    "            'config': architecture_config,\n",
    "            'params': config\n",
    "        })\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0909af",
   "metadata": {},
   "source": [
    "# Train generated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8834bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"Initialize the distributed environment\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up distributed processes\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def train_model(rank, world_size, model_config, X_train, y_train, X_val, y_val, \n",
    "               epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"Train a model on a specific GPU\"\"\"\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # Split data across GPUs\n",
    "    train_size = len(X_train)\n",
    "    indices = list(range(rank, train_size, world_size))\n",
    "    \n",
    "    # Convert to tensors and move to current GPU\n",
    "    X_train_tensor = torch.FloatTensor(X_train[indices]).to(rank)\n",
    "    y_train_tensor = torch.FloatTensor(y_train[indices]).to(rank)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(rank)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(rank)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size // world_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model and wrap with DDP\n",
    "    model = GeneratedModel(\n",
    "        input_size=X_train.shape[1],\n",
    "        output_size=y_train.shape[1],\n",
    "        architecture_config=model_config['config']\n",
    "    ).to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    progress_bar = tqdm(range(epochs), desc=f'GPU {rank}: {model_config[\"name\"]}', position=rank)\n",
    "    \n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = np.mean(batch_losses)\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation (only on rank 0)\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs, _ = model(X_val_tensor)\n",
    "                val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "                val_loss_history.append(val_loss)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'train_loss': f'{train_loss:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Final evaluation (only on rank 0)\n",
    "    if rank == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Gather predictions from all GPUs\n",
    "            all_train_preds = []\n",
    "            all_val_preds = []\n",
    "            \n",
    "            for i in range(world_size):\n",
    "                idx = list(range(i, len(X_train), world_size))\n",
    "                X_part = torch.FloatTensor(X_train[idx]).to(rank)\n",
    "                preds, _ = model(X_part)\n",
    "                all_train_preds.append(preds.cpu().numpy())\n",
    "                \n",
    "                X_val_part = torch.FloatTensor(X_val).to(rank)\n",
    "                val_preds, _ = model(X_val_part)\n",
    "                all_val_preds.append(val_preds.cpu().numpy())\n",
    "            \n",
    "            train_preds = np.concatenate(all_train_preds)\n",
    "            val_preds = np.concatenate(all_val_preds)\n",
    "            \n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "            train_mae = mean_absolute_error(y_train, train_preds)\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "            val_mae = mean_absolute_error(y_val, val_preds)\n",
    "        \n",
    "        result = {\n",
    "            'model': model.module,\n",
    "            'train_loss_history': train_loss_history,\n",
    "            'val_loss_history': val_loss_history,\n",
    "            'train_rmse': train_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'val_rmse': val_rmse,\n",
    "            'val_mae': val_mae,\n",
    "            'model_name': model_config['name'],\n",
    "            'params': model_config['params']\n",
    "        }\n",
    "    else:\n",
    "        result = None\n",
    "    \n",
    "    cleanup()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a498d6c",
   "metadata": {},
   "source": [
    "# Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52d69ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_gpu_grid_search(X_train, y_train, X_val, y_val, search_space, epochs=200):\n",
    "    \"\"\"Run grid search using all available GPUs\"\"\"\n",
    "    # Generate all model configurations\n",
    "    model_configs = generate_model_configs(search_space)\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"Found {world_size} GPUs. Using all of them for training.\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Set start method only if not already set\n",
    "    if mp.get_start_method(allow_none=True) is None:\n",
    "        mp.set_start_method('spawn')\n",
    "    \n",
    "    for config in model_configs:\n",
    "        print(f\"\\nTraining {config['name']} with config:\")\n",
    "        print(config['params'])\n",
    "        \n",
    "        # Launch training on all GPUs\n",
    "        mp.spawn(\n",
    "            train_model,\n",
    "            args=(world_size, config, X_train, y_train, X_val, y_val, epochs),\n",
    "            nprocs=world_size,\n",
    "            join=True\n",
    "        )\n",
    "        \n",
    "        # Only rank 0 returns results\n",
    "        if torch.cuda.current_device() == 0:\n",
    "            result = train_model(\n",
    "                0, world_size, config, X_train, y_train, X_val, y_val, epochs\n",
    "            )\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_programmatic_results(results):\n",
    "    \"\"\"Plot results from programmatic grid search\"\"\"\n",
    "    num_models = len(results)\n",
    "    cols = 2\n",
    "    rows = math.ceil(num_models / cols)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * rows))\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.plot(result['train_loss_history'], label='Train Loss')\n",
    "        plt.plot(result['val_loss_history'], label='Validation Loss')\n",
    "        \n",
    "        params = result['params']\n",
    "        title = (f\"{result['model_name']}\\n\"\n",
    "                f\"Layers: {params['num_layers']}, Size: {params['layer_size']}\\n\"\n",
    "                f\"Act: {params['activation']}, BN: {params['batch_norm']}\\n\"\n",
    "                f\"Dropout: {params['dropout'] if params['use_dropout'] else 'No'}\\n\"\n",
    "                f\"Val RMSE: {result['val_rmse']:.2f}, Val MAE: {result['val_mae']:.2f}\")\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"{:<10} {:<10} {:<10} {:<15} {:<15} {:<15} {:<15} {:<15}\".format(\n",
    "        \"Model\", \"Layers\", \"Size\", \"Activation\", \"BatchNorm\", \"Dropout\", \n",
    "        \"Val RMSE\", \"Val MAE\"))\n",
    "    \n",
    "    for result in results:\n",
    "        params = result['params']\n",
    "        print(\"{:<10} {:<10} {:<10} {:<15} {:<15} {:<15} {:<15.4f} {:<15.4f}\".format(\n",
    "            result['model_name'],\n",
    "            params['num_layers'],\n",
    "            params['layer_size'],\n",
    "            params['activation'],\n",
    "            \"Yes\" if params['batch_norm'] else \"No\",\n",
    "            f\"{params['dropout']}\" if params['use_dropout'] else \"No\",\n",
    "            result['val_rmse'],\n",
    "            result['val_mae']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0a122",
   "metadata": {},
   "source": [
    "# Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dde7a2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 GPUs. Using all of them for training.\n",
      "\n",
      "Training Model_1 with config:\n",
      "{'num_layers': 3, 'layer_size': 128, 'activation': 'relu', 'batch_norm': True, 'use_dropout': True, 'dropout': 0.3, 'attention': False, 'uncertainty_estimation': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=149, pipe_handle=151)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/admindi/miniforge3/envs/location-env/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/home/admindi/miniforge3/envs/location-env/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'train_model' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     24\u001b[39m search_space = {\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_layers\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m],\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlayer_size\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33muncertainty_estimation\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[32m     33\u001b[39m }\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Run the multi-GPU grid search\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m results = \u001b[43mrun_multi_gpu_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrun_multi_gpu_grid_search\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, search_space, epochs)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(config[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Launch training on all GPUs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Only rank 0 returns results\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.current_device() == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/site-packages/torch/multiprocessing/spawn.py:340\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    334\u001b[39m     msg = (\n\u001b[32m    335\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/site-packages/torch/multiprocessing/spawn.py:280\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m start_parallel:\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nprocs):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         idx, process, tf_name = \u001b[43mstart_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m         error_files[idx] = tf_name\n\u001b[32m    282\u001b[39m         processes[idx] = process\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/site-packages/torch/multiprocessing/spawn.py:275\u001b[39m, in \u001b[36mstart_processes.<locals>.start_process\u001b[39m\u001b[34m(i)\u001b[39m\n\u001b[32m    269\u001b[39m os.unlink(tf.name)\n\u001b[32m    270\u001b[39m process = mp.Process(\n\u001b[32m    271\u001b[39m     target=_wrap,\n\u001b[32m    272\u001b[39m     args=(fn, i, args, tf.name),\n\u001b[32m    273\u001b[39m     daemon=daemon,\n\u001b[32m    274\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m i, process, tf.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/location-env/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Get datasets from all collections\n",
    "datasets = [\n",
    "    get_dataset(\"wifi_data_reto_grande\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_pequeno\", \"wifi_data_db\"),\n",
    "    get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\")\n",
    "]\n",
    "\n",
    "# Combine all datasets into one array\n",
    "combined_data = combine_arrays(datasets)\n",
    "\n",
    "# Shuffle the combined data\n",
    "shuffled_data = shuffle_array(combined_data)\n",
    "\n",
    "# Split into features and labels\n",
    "#training_x, training_y = split_combined_data(shuffled_data)\n",
    "#validation_x, validation_y = split_combined_data(get_dataset(\"wifi_data_reto_medio\", \"wifi_data_db\"))\n",
    "global_array_x, global_array_y = split_combined_data(shuffled_data)\n",
    "\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(global_array_x, global_array_y, test_size=0.2, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = training_x, validation_x, training_y, validation_y\n",
    "\n",
    "search_space = {\n",
    "    'num_layers': [3, 4],\n",
    "    'layer_size': [128, 256],\n",
    "    'activation': ['relu', 'leaky_relu'],\n",
    "    'batch_norm': [True, False],\n",
    "    'use_dropout': [True],\n",
    "    'dropout': [0.3],\n",
    "    'attention': [False],\n",
    "    'uncertainty_estimation': [False]\n",
    "}\n",
    "\n",
    "# Run the multi-GPU grid search\n",
    "results = run_multi_gpu_grid_search(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    search_space=search_space,\n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed94e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (only need to do this once)\n",
    "if torch.cuda.current_device() == 0:\n",
    "    plot_programmatic_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "location-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
