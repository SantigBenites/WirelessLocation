{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5308758b",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Files:\n",
    "- config.json — architecture + basic metadata\n",
    "- weights.pth — raw PyTorch state_dict (CPU tensors)\n",
    "- model_package.pth — single-file package with arch_config + state_dict + meta\n",
    "- {os.path.basename(ckpt_path)} — original Lightning checkpoint (copied for provenance)\n",
    "{('- model_torchscript.pt — TorchScript' if DO_TORCHSCRIPT and 'torchscript_path' in results else '')}\n",
    "{('- model.onnx — ONNX' if DO_ONNX and 'onnx_path' in results else '')}\n",
    "\n",
    "Load from model_package.pth:\n",
    "```python\n",
    "import torch, json\n",
    "from model_generation import GeneratedModel\n",
    "\n",
    "pkg = torch.load(\"model_package.pth\", map_location=\"cpu\")\n",
    "arch = pkg[\"arch_config\"]\n",
    "state = pkg[\"state_dict\"]\n",
    "meta = pkg[\"meta\"]\n",
    "\n",
    "# If you know input/output size:\n",
    "model = GeneratedModel(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, architecture_config=arch)\n",
    "# Strip 'model.' keys if present\n",
    "state = {{ (k.split('model.',1)[1] if k.startswith('model.') else k): v for k,v in state.items() }}\n",
    "model.load_state_dict(state, strict=False)\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f0b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, shutil, argparse\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ----------------------\n",
    "# Config (edit defaults or pass args)\n",
    "# ----------------------\n",
    "CSV_PATH = \"/home/admindi/sbenites/WirelessLocation/validation/model_per_dataset_validation/wandb_export_2025-08-22T14_23_07.979+01_00.csv\"\n",
    "SEARCH_ROOTS = [\n",
    "    \"/home/admindi/sbenites/WirelessLocation\",\n",
    "    \"/mnt/data\",\n",
    "]\n",
    "EXPORT_ROOT = \"/home/admindi/sbenites/WirelessLocation/validation/model_per_dataset_validation/full_models\"\n",
    "\n",
    "# What to produce per run\n",
    "WRITE_PACKAGE = True            # model_package.pth containing {'arch_config','state_dict','meta'}\n",
    "WRITE_RAW_STATE_DICT = True     # weights.pth (state_dict only, CPU)\n",
    "COPY_CKPT = True                # copy original lightning .ckpt for provenance\n",
    "DO_TORCHSCRIPT = False          # also export torchscript (requires model construction)\n",
    "DO_ONNX = False                 # also export onnx (requires model construction)\n",
    "\n",
    "# If you want TorchScript/ONNX, either set INPUT_SIZE/OUTPUT_SIZE, or enable INFER_DIMS_FROM_DB\n",
    "INPUT_SIZE = None               # e.g., 256\n",
    "OUTPUT_SIZE = None              # e.g., 2\n",
    "INFER_DIMS_FROM_DB = False\n",
    "DB_NAME = \"wifi_fingerprinting_data\"\n",
    "SUBSET_FOR_DIMS = [\"reto_grande_indoor\"]  # minimal one collection to probe dims\n",
    "\n",
    "# ----------------------\n",
    "# CSV -> arch_config mapping\n",
    "# ----------------------\n",
    "CSV_KEY_MAP = {\n",
    "    \"architecture.num_conv_layers\": \"num_conv_layers\",\n",
    "    \"architecture.filters_per_layer\": \"filters_per_layer\",\n",
    "    \"architecture.kernel_size\": \"kernel_size\",\n",
    "    \"architecture.stride\": \"stride\",\n",
    "    \"architecture.padding\": \"padding\",\n",
    "    \"architecture.activation\": \"activation\",\n",
    "    \"architecture.batch_norm\": \"batch_norm\",\n",
    "    \"architecture.dropout\": \"dropout\",\n",
    "    \"architecture.pooling_type\": \"pooling_type\",\n",
    "    \"architecture.pool_size\": \"pool_size\",\n",
    "    \"architecture.residual_connections\": \"residual_connections\",\n",
    "    \"architecture.learning_rate\": \"learning_rate\",\n",
    "    \"architecture.weight_decay\": \"weight_decay\",\n",
    "    \"architecture.optimizer\": \"optimizer\",\n",
    "    \"architecture.batch_size\": \"batch_size\",\n",
    "    \"architecture.normalization\": \"normalization\",\n",
    "    \"architecture.initialization\": \"initialization\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f30842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_bool(v):\n",
    "    if isinstance(v, bool): return v\n",
    "    if isinstance(v, (int, float)): return bool(v)\n",
    "    if isinstance(v, str):\n",
    "        vl = v.strip().lower()\n",
    "        if vl in (\"true\",\"yes\",\"1\"): return True\n",
    "        if vl in (\"false\",\"no\",\"0\",\"\"): return False\n",
    "    return False\n",
    "\n",
    "def row_to_arch_config(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    cfg = {}\n",
    "    present = False\n",
    "    for csv_key, cfg_key in CSV_KEY_MAP.items():\n",
    "        if csv_key in row and pd.notnull(row[csv_key]):\n",
    "            val = row[csv_key]; present = True\n",
    "            if cfg_key in {\"num_conv_layers\",\"filters_per_layer\",\"kernel_size\",\"stride\",\"pool_size\",\"batch_size\"}:\n",
    "                try: val = int(float(val))\n",
    "                except: pass\n",
    "            elif cfg_key in {\"dropout\",\"learning_rate\",\"weight_decay\"}:\n",
    "                try: val = float(val)\n",
    "                except: pass\n",
    "            elif cfg_key in {\"batch_norm\",\"residual_connections\"}:\n",
    "                val = _coerce_bool(val)\n",
    "            elif cfg_key in {\"activation\",\"optimizer\",\"padding\",\"pooling_type\",\"normalization\",\"initialization\"}:\n",
    "                val = str(val).strip().lower()\n",
    "            cfg[cfg_key] = val\n",
    "    if not present: return None\n",
    "    if \"dropout\" in cfg:\n",
    "        cfg.setdefault(\"use_dropout\", float(cfg[\"dropout\"]) > 0.0)\n",
    "    cfg.setdefault(\"normalization\", \"none\")\n",
    "    cfg.setdefault(\"initialization\", \"default\")\n",
    "    return cfg\n",
    "\n",
    "def pick_model_name(row: pd.Series, cfg: Dict[str, Any]) -> str:\n",
    "    for k in [\"Name\",\"name\",\"run_name\",\"id\",\"Run\"]:\n",
    "        if k in row and pd.notnull(row[k]):\n",
    "            return sanitize(str(row[k]))\n",
    "    # fallback hash\n",
    "    keys = [\"num_conv_layers\",\"filters_per_layer\",\"kernel_size\",\"stride\",\"pooling_type\",\"residual_connections\"]\n",
    "    sig = \"-\".join(str(cfg.get(k,\"\")) for k in keys)\n",
    "    return sanitize(f\"model_{abs(hash(sig))%100000}\")\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s.strip())\n",
    "\n",
    "def find_ckpt_for_name(name: str) -> Optional[str]:\n",
    "    for root in SEARCH_ROOTS:\n",
    "        pattern = os.path.join(root, \"**\", f\"{name}.ckpt\")\n",
    "        matches = [p for p in glob(pattern) if os.path.isfile(p)]\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def to_cpu_state_dict(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    return {k: v.detach().cpu() for k, v in sd.items()}\n",
    "\n",
    "def export_one(name: str, ckpt_path: str, arch_config: Dict[str, Any], out_dir: str, dims: Optional[tuple] = None) -> Dict[str, Any]:\n",
    "    ensure_dir(out_dir)\n",
    "    # Load lightning checkpoint\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "    state_dict = to_cpu_state_dict(state_dict)\n",
    "\n",
    "    # Metadata\n",
    "    meta = {\n",
    "        \"run_name\": name,\n",
    "        \"ckpt_path\": ckpt_path,\n",
    "        \"exported_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"git_commit\": os.getenv(\"GIT_COMMIT\", \"\"),\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"package_format_version\": 1,\n",
    "    }\n",
    "\n",
    "    results = {\"name\": name, \"out_dir\": out_dir}\n",
    "\n",
    "    # Write config.json\n",
    "    with open(os.path.join(out_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"arch_config\": arch_config, \"meta\": meta}, f, indent=2)\n",
    "\n",
    "    # Write raw weights.pth\n",
    "    if WRITE_RAW_STATE_DICT:\n",
    "        weights_path = os.path.join(out_dir, \"weights.pth\")\n",
    "        torch.save(state_dict, weights_path)\n",
    "        results[\"weights_path\"] = weights_path\n",
    "\n",
    "    # Write single package with arch + weights + meta\n",
    "    if WRITE_PACKAGE:\n",
    "        package = {\"arch_config\": arch_config, \"state_dict\": state_dict, \"meta\": meta}\n",
    "        pkg_path = os.path.join(out_dir, \"model_package.pth\")\n",
    "        torch.save(package, pkg_path)\n",
    "        results[\"package_path\"] = pkg_path\n",
    "\n",
    "    # Copy original ckpt\n",
    "    if COPY_CKPT:\n",
    "        dst = os.path.join(out_dir, os.path.basename(ckpt_path))\n",
    "        try:\n",
    "            shutil.copy2(ckpt_path, dst)\n",
    "            results[\"ckpt_copy\"] = dst\n",
    "        except Exception as e:\n",
    "            results[\"ckpt_copy_error\"] = str(e)\n",
    "\n",
    "    # Optional TorchScript/ONNX (requires model construction)\n",
    "    if (DO_TORCHSCRIPT or DO_ONNX) and dims is not None:\n",
    "        try:\n",
    "            from model_generation import GeneratedModel\n",
    "            input_size, output_size = dims\n",
    "            model = GeneratedModel(input_size=input_size, output_size=output_size, architecture_config=arch_config)\n",
    "            # The Lightning state_dict usually prefixes with 'model.' — strip if necessary\n",
    "            cleaned = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if k.startswith(\"model.\"):\n",
    "                    cleaned[k[len(\"model.\"):]] = v\n",
    "                else:\n",
    "                    cleaned[k] = v\n",
    "            missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "            if missing:\n",
    "                print(f\"[{name}] Missing keys on model load: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
    "            if unexpected:\n",
    "                print(f\"[{name}] Unexpected keys on model load: {unexpected[:8]}{'...' if len(unexpected)>8 else ''}\")\n",
    "            model.eval()\n",
    "\n",
    "            dummy = torch.randn(1, input_size)\n",
    "            if DO_TORCHSCRIPT:\n",
    "                ts = torch.jit.trace(model, dummy)\n",
    "                ts_path = os.path.join(out_dir, \"model_torchscript.pt\")\n",
    "                ts.save(ts_path)\n",
    "                results[\"torchscript_path\"] = ts_path\n",
    "            if DO_ONNX:\n",
    "                onnx_path = os.path.join(out_dir, \"model.onnx\")\n",
    "                torch.onnx.export(model, dummy, onnx_path, input_names=[\"input\"], output_names=[\"output\"], opset_version=17)\n",
    "                results[\"onnx_path\"] = onnx_path\n",
    "        except Exception as e:\n",
    "            results[\"jit_onnx_error\"] = str(e)\n",
    "\n",
    "            \n",
    "    with open(os.path.join(out_dir, \"README.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    return results\n",
    "\n",
    "def glob(pattern: str) -> List[str]:\n",
    "    import glob as _g\n",
    "    return _g.glob(pattern, recursive=True)\n",
    "\n",
    "def infer_dims_from_db() -> Optional[tuple]:\n",
    "    try:\n",
    "        from data_processing import get_dataset, combine_arrays, shuffle_array, split_combined_data\n",
    "        datasets = [get_dataset(name, DB_NAME) for name in SUBSET_FOR_DIMS]\n",
    "        combined = combine_arrays(datasets)\n",
    "        X, y = split_combined_data(combined)\n",
    "        return (X.shape[1], y.shape[1])\n",
    "    except Exception as e:\n",
    "        print(\"Failed to infer dims from DB:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51fb5f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in CSV: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2116317/3118488348.py:72: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"exported_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdoor_indoor_and_garage_run3_depth7_model7: error: name 'readme' is not defined\n",
      "all_data_run1_depth5_model1: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run0_depth0_model5: error: name 'readme' is not defined\n",
      "all_data_run3_depth1_model7: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run1_depth3_model5: error: name 'readme' is not defined\n",
      "all_data_run4_depth9_model9: error: name 'readme' is not defined\n",
      "all_data_run7_depth2_model7: error: name 'readme' is not defined\n",
      "all_data_run4_depth2_model5: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run2_depth0_model11: error: name 'readme' is not defined\n",
      "all_data_run1_depth5_model7: error: name 'readme' is not defined\n",
      "all_data_run3_depth2_model5: error: name 'readme' is not defined\n",
      "all_data_run0_depth7_model5: error: name 'readme' is not defined\n",
      "all_data_run0_depth8_model5: error: name 'readme' is not defined\n",
      "all_data_run6_depth0_model2: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run4_depth1_model1: error: name 'readme' is not defined\n",
      "all_data_run9_depth8_model8: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run7_depth9_model0: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run9_depth3_model7: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run0_depth0_model11: error: name 'readme' is not defined\n",
      "all_data_run2_depth3_model0: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run4_depth6_model0: error: name 'readme' is not defined\n",
      "all_data_run2_depth5_model8: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run2_depth7_model4: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run3_depth2_model10: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run8_depth9_model5: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run6_depth0_model6: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run1_depth9_model0: error: name 'readme' is not defined\n",
      "all_data_run5_depth5_model3: error: name 'readme' is not defined\n",
      "all_data_run8_depth1_model8: error: name 'readme' is not defined\n",
      "outdoor_indoor_and_garage_run5_depth1_model10: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run6_depth3_model3: error: name 'readme' is not defined\n",
      "garage_only_run0_depth6_model0: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run8_depth7_model3: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run2_depth3_model0: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run0_depth6_model1: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run4_depth8_model7: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run3_depth6_model0: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run4_depth9_model7: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run5_depth4_model5: error: name 'readme' is not defined\n",
      "garage_only_run6_depth9_model3: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run0_depth5_model1: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run1_depth6_model4: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run2_depth4_model2: error: name 'readme' is not defined\n",
      "garage_only_run8_depth8_model4: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run9_depth4_model9: error: name 'readme' is not defined\n",
      "garage_only_run4_depth7_model10: error: name 'readme' is not defined\n",
      "garage_only_run3_depth7_model0: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run0_depth6_model1: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run0_depth2_model7: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run4_depth5_model5: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run7_depth5_model11: error: name 'readme' is not defined\n",
      "garage_only_run1_depth2_model2: error: name 'readme' is not defined\n",
      "garage_only_run3_depth0_model3: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run3_depth6_model7: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run6_depth8_model4: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run5_depth4_model0: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run4_depth6_model4: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run7_depth5_model5: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run1_depth6_model11: error: name 'readme' is not defined\n",
      "garage_only_run0_depth9_model4: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run1_depth7_model7: error: name 'readme' is not defined\n",
      "outdoor_only_run7_depth4_model1: error: name 'readme' is not defined\n",
      "garage_only_run2_depth8_model7: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run3_depth2_model0: error: name 'readme' is not defined\n",
      "garage_only_run2_depth3_model5: error: name 'readme' is not defined\n",
      "garage_only_run5_depth0_model2: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run3_depth8_model0: error: name 'readme' is not defined\n",
      "outdoor_only_run4_depth2_model1: error: name 'readme' is not defined\n",
      "garage_only_run4_depth4_model1: error: name 'readme' is not defined\n",
      "outdoor_only_run2_depth6_model5: error: name 'readme' is not defined\n",
      "outdoor_only_run1_depth1_model4: error: name 'readme' is not defined\n",
      "outdoor_only_run9_depth5_model3: error: name 'readme' is not defined\n",
      "outdoor_only_run3_depth1_model4: error: name 'readme' is not defined\n",
      "outdoor_only_run0_depth4_model11: error: name 'readme' is not defined\n",
      "garage_only_run9_depth5_model10: error: name 'readme' is not defined\n",
      "garage_only_run7_depth4_model10: error: name 'readme' is not defined\n",
      "outdoor_only_run8_depth9_model2: error: name 'readme' is not defined\n",
      "outdoor_only_run1_depth9_model4: error: name 'readme' is not defined\n",
      "outdoor_only_run2_depth3_model9: error: name 'readme' is not defined\n",
      "outdoor_only_run0_depth6_model2: error: name 'readme' is not defined\n",
      "garage_only_run1_depth8_model0: error: name 'readme' is not defined\n",
      "outdoor_only_run6_depth6_model6: error: name 'readme' is not defined\n",
      "outdoor_only_run3_depth0_model0: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run2_depth0_model1: error: name 'readme' is not defined\n",
      "outdoor_only_run4_depth8_model2: error: name 'readme' is not defined\n",
      "outdoor_only_run5_depth6_model8: error: name 'readme' is not defined\n",
      "indoor_only_run9_depth0_model6: error: name 'readme' is not defined\n",
      "indoor_only_run1_depth3_model10: error: name 'readme' is not defined\n",
      "indoor_only_run8_depth5_model0: error: name 'readme' is not defined\n",
      "indoor_only_run0_depth3_model1: error: name 'readme' is not defined\n",
      "indoor_only_run1_depth6_model4: error: name 'readme' is not defined\n",
      "indoor_only_run3_depth1_model1: error: name 'readme' is not defined\n",
      "outdoor_and_indoor_run1_depth2_model3: error: name 'readme' is not defined\n",
      "indoor_only_run0_depth5_model9: error: name 'readme' is not defined\n",
      "indoor_only_run6_depth1_model4: error: name 'readme' is not defined\n",
      "indoor_only_run3_depth6_model7: error: name 'readme' is not defined\n",
      "indoor_only_run7_depth5_model0: error: name 'readme' is not defined\n",
      "indoor_only_run4_depth6_model6: error: name 'readme' is not defined\n",
      "indoor_only_run2_depth8_model5: error: name 'readme' is not defined\n",
      "indoor_only_run4_depth7_model11: error: name 'readme' is not defined\n",
      "indoor_only_run2_depth7_model1: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run2_depth3_model4: error: name 'readme' is not defined\n",
      "indoor_only_run5_depth5_model4: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run9_depth1_model10: error: name 'readme' is not defined\n",
      "outdoor_and_garage_run8_depth2_model1: error: name 'readme' is not defined\n",
      "Wrote index: /home/admindi/sbenites/WirelessLocation/validation/model_per_dataset_validation/full_models/export_index.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "os.makedirs(EXPORT_ROOT, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Rows in CSV:\", len(df))\n",
    "\n",
    "dims = None\n",
    "if DO_TORCHSCRIPT or DO_ONNX:\n",
    "    if INPUT_SIZE and OUTPUT_SIZE:\n",
    "        dims = (int(INPUT_SIZE), int(OUTPUT_SIZE))\n",
    "    elif INFER_DIMS_FROM_DB:\n",
    "        dims = infer_dims_from_db()\n",
    "    print(\"Dims for scripted exports:\", dims)\n",
    "\n",
    "overview = []\n",
    "for _, row in df.iterrows():\n",
    "    cfg = row_to_arch_config(row)\n",
    "    if not isinstance(cfg, dict):\n",
    "        continue\n",
    "    name = pick_model_name(row, cfg)\n",
    "    ckpt = find_ckpt_for_name(name)\n",
    "    found = bool(ckpt)\n",
    "    out_dir = os.path.join(EXPORT_ROOT, name)\n",
    "    rec = {\"name\": name, \"found_ckpt\": found, \"ckpt_path\": ckpt, \"out_dir\": out_dir}\n",
    "    if found:\n",
    "        try:\n",
    "            res = export_one(name, ckpt, cfg, out_dir, dims=dims)\n",
    "            rec.update({k:v for k,v in res.items() if k not in {\"name\",\"out_dir\"}})\n",
    "            rec[\"status\"] = \"exported\"\n",
    "        except Exception as e:\n",
    "            rec[\"status\"] = f\"error: {e}\"\n",
    "    else:\n",
    "        rec[\"status\"] = \"missing_ckpt\"\n",
    "    overview.append(rec)\n",
    "    print(f\"{name}: {rec['status']}\")\n",
    "\n",
    "idx = pd.DataFrame(overview)\n",
    "idx_path = os.path.join(EXPORT_ROOT, \"export_index.csv\")\n",
    "idx.to_csv(idx_path, index=False)\n",
    "print(\"Wrote index:\", idx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "location-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
