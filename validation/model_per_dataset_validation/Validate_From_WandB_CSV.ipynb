{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d48089",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Validate Models from W&B CSV (Reconstruct + Evaluate on Collections)\n",
    "\n",
    "This notebook:\n",
    "1. Reads your **W&B CSV export** with run metadata/hparams.\n",
    "2. Reconstructs each model's **architecture config** from the CSV (JSON-like columns or flattened keys).\n",
    "3. Locates the corresponding **`.ckpt`** (by `name` â†’ `<name>.ckpt`).\n",
    "4. Writes a **sidecar** `<name>.arch.json` next to each checkpoint (future-proof).\n",
    "5. **Validates** each reconstructed model over your chosen collections/subsets.\n",
    "\n",
    "> Works entirely with the files you already have (CSV + checkpoints + your project code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab289c",
   "metadata": {},
   "source": [
    "## ðŸ”§ Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a082fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your W&B CSV export\n",
    "CSV_PATH = \"/mnt/data/wandb_export_2025-08-22T14_23_07.979+01_00.csv\"  # change if needed\n",
    "\n",
    "# Where to search for checkpoints (add more roots as needed)\n",
    "SEARCH_ROOTS = [\n",
    "    \"/home/admindi/sbenites/WirelessLocation\",\n",
    "    \"/mnt/data\",\n",
    "]\n",
    "\n",
    "# Optional: if your CSV already has a 'ckpt_path' column, set this True to trust it first\n",
    "TRUST_CKPT_PATH_COLUMN = True\n",
    "\n",
    "# Validation subsets\n",
    "SUBSETS = [\"garage\", \"outdoor\", \"indoor\"]  # or [\"all\"] or include \"collections\"\n",
    "# Only used if you include \"collections\" in SUBSETS\n",
    "COLLECTIONS = [\n",
    "    # \"reto_grande_outdoor\", \"equilatero_grande_outdoor\"\n",
    "]\n",
    "\n",
    "# MongoDB / data loading\n",
    "DB_NAME = \"wifi_fingerprinting_data\"\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "# Project import roots (so the notebook can import your modules)\n",
    "PROJECT_PATHS = [\n",
    "    \"/home/admindi/sbenites/WirelessLocation\",\n",
    "    \"/mnt/data\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00e28a",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, json, ast, math, re\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "for p in PROJECT_PATHS:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "from data_processing import get_dataset, combine_arrays, shuffle_array, split_combined_data\n",
    "from model_generation import GeneratedModel\n",
    "from gpu_fucntion import LightningWrapper\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e55c07",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Collections & Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_COLLECTIONS = [\n",
    "    \"equilatero_grande_garage\",\n",
    "    \"equilatero_grande_outdoor\",\n",
    "    \"equilatero_medio_garage\",\n",
    "    \"equilatero_medio_outdoor\",\n",
    "    \"isosceles_grande_indoor\",\n",
    "    \"isosceles_grande_outdoor\",\n",
    "    \"isosceles_medio_outdoor\",\n",
    "    \"obtusangulo_grande_outdoor\",\n",
    "    \"obtusangulo_pequeno_outdoor\",\n",
    "    \"reto_grande_garage\",\n",
    "    \"reto_grande_indoor\",\n",
    "    \"reto_grande_outdoor\",\n",
    "    \"reto_medio_garage\",\n",
    "    \"reto_medio_outdoor\",\n",
    "    \"reto_n_quadrado_grande_indoor\",\n",
    "    \"reto_n_quadrado_grande_outdoor\",\n",
    "    \"reto_n_quadrado_pequeno_outdoor\",\n",
    "    \"reto_pequeno_garage\",\n",
    "    \"reto_pequeno_outdoor\",\n",
    "]\n",
    "\n",
    "def group_by_location(collections: List[str], locations: List[str]) -> List[str]:\n",
    "    return [name for name in collections if any(loc in name for loc in locations)]\n",
    "\n",
    "SUBSET_MAP = {\n",
    "    \"garage\": group_by_location(ALL_COLLECTIONS, [\"garage\"]),\n",
    "    \"outdoor\": group_by_location(ALL_COLLECTIONS, [\"outdoor\"]),\n",
    "    \"indoor\": group_by_location(ALL_COLLECTIONS, [\"indoor\"]),\n",
    "    \"all\": ALL_COLLECTIONS,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be6972",
   "metadata": {},
   "source": [
    "## ðŸ§© Parse Architecture Config From CSV Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_jsonish(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Try to parse a JSON/dict-like string to a dict.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    s = text.strip()\n",
    "    # Common cleanup: single quotes -> double quotes, True/False/None -> JSON\n",
    "    candidates = [s]\n",
    "    candidates.append(s.replace(\"'\", '\"'))\n",
    "    repl = (('None','null'), ('True','true'), ('False','false'))\n",
    "    s2 = s\n",
    "    for a,b in repl:\n",
    "        s2 = re.sub(r'\\b'+a+r'\\b', b, s2)\n",
    "    candidates.append(s2)\n",
    "    candidates.append(s2.replace(\"'\", '\"'))\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return json.loads(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            obj = ast.literal_eval(c)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def set_in_nested(d: Dict[str, Any], path_parts: List[Any], value: Any):\n",
    "    cur = d\n",
    "    for i, key in enumerate(path_parts):\n",
    "        is_last = (i == len(path_parts) - 1)\n",
    "        # numeric indices indicate lists\n",
    "        if isinstance(key, int):\n",
    "            if not isinstance(cur, list):\n",
    "                # convert current level to list\n",
    "                cur_list = []\n",
    "                # if cur was empty dict, replace; if dict with content, we can't merge safely\n",
    "                # so just replace (best effort)\n",
    "                cur.clear()\n",
    "                cur_list = []\n",
    "                cur = cur_list\n",
    "            # extend list to index\n",
    "            while len(cur) <= key:\n",
    "                cur.append({})\n",
    "            if is_last:\n",
    "                cur[key] = value\n",
    "            else:\n",
    "                if not isinstance(cur[key], (dict, list)):\n",
    "                    cur[key] = {}\n",
    "                cur = cur[key]\n",
    "        else:\n",
    "            if is_last:\n",
    "                cur[key] = value\n",
    "            else:\n",
    "                if key not in cur or not isinstance(cur[key], (dict, list)):\n",
    "                    cur[key] = {}\n",
    "                cur = cur[key]\n",
    "\n",
    "def parse_flattened_to_config(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Reconstruct a nested config dict from columns like 'config.layers.0.filters' = 64.\"\"\"\n",
    "    colnames = list(row.index)\n",
    "    # Candidate prefixes that indicate config-ish fields\n",
    "    prefixes = [\"config.\", \"architecture_config.\", \"arch_config.\", \"model_config.\", \"architecture.\", \"cnn_config.\"]\n",
    "\n",
    "    candidates = [c for c in colnames if any(c.startswith(p) for p in prefixes)]\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    cfg: Dict[str, Any] = {}\n",
    "    for c in candidates:\n",
    "        val = row[c]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        # Try to coerce to python types\n",
    "        if isinstance(val, str):\n",
    "            if val.lower() in (\"true\",\"false\"):\n",
    "                val = (val.lower() == \"true\")\n",
    "            else:\n",
    "                # try number\n",
    "                try:\n",
    "                    if \".\" in val:\n",
    "                        val = float(val)\n",
    "                        if val.is_integer():\n",
    "                            val = int(val)\n",
    "                    else:\n",
    "                        val = int(val)\n",
    "                except Exception:\n",
    "                    # try jsonish for lists/dicts\n",
    "                    parsed = parse_jsonish(val)\n",
    "                    val = parsed if parsed is not None else val\n",
    "\n",
    "        # Build path parts: split by dot, and convert '[i]' to index\n",
    "        parts = []\n",
    "        for part in c.split(\".\"):\n",
    "            m = re.match(r\"^(.*?)(\\[(\\d+)\\])?$\", part)\n",
    "            if not m:\n",
    "                parts.append(part)\n",
    "                continue\n",
    "            name, _, idx = m.groups()\n",
    "            if name:\n",
    "                parts.append(name)\n",
    "            if idx is not None:\n",
    "                parts.append(int(idx))\n",
    "\n",
    "        # Remove leading prefix token like 'config'\n",
    "        if parts and parts[0] in (\"config\",\"architecture_config\",\"arch_config\",\"model_config\",\"architecture\",\"cnn_config\"):\n",
    "            parts = parts[1:]\n",
    "        if not parts:\n",
    "            continue\n",
    "\n",
    "        set_in_nested(cfg, parts, val)\n",
    "    return cfg if cfg else None\n",
    "\n",
    "def row_to_arch_config(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    # 1) Direct JSON-like columns\n",
    "    direct_cols = [\n",
    "        \"architecture_config\", \"arch_config\", \"config\", \"model_config\",\n",
    "        \"architecture\", \"cnn_config\", \"config_json\", \"hparams\", \"hyperparameters\",\n",
    "    ]\n",
    "    for col in direct_cols:\n",
    "        if col in row and isinstance(row[col], str):\n",
    "            cfg = parse_jsonish(row[col])\n",
    "            if isinstance(cfg, dict):\n",
    "                return cfg\n",
    "\n",
    "    # 2) Flattened columns\n",
    "    cfg = parse_flattened_to_config(row)\n",
    "    if isinstance(cfg, dict) and cfg:\n",
    "        return cfg\n",
    "\n",
    "    return None\n",
    "\n",
    "def pick_model_name(row: pd.Series) -> Optional[str]:\n",
    "    \"\"\"Try common columns for checkpoint stem (model name).\"\"\"\n",
    "    for key in [\"name\",\"model_name\",\"ckpt_stem\",\"run_name\",\"id\",\"slug\"]:\n",
    "        if key in row and isinstance(row[key], str) and row[key].strip():\n",
    "            # sanitize forbidden path chars\n",
    "            base = os.path.basename(row[key].strip())\n",
    "            stem = os.path.splitext(base)[0]\n",
    "            return stem\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c08d8",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load CSV and Build (name â†’ config, ckpt) list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35689d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "print(\"CSV rows:\", len(df_raw))\n",
    "print(\"CSV columns:\", list(df_raw.columns)[:30], \"...\")\n",
    "\n",
    "entries = []  # list of dicts: {name, arch_config, ckpt_path}\n",
    "\n",
    "def find_ckpt_for_name(name: str) -> Optional[str]:\n",
    "    # If CSV contains an explicit ckpt path, prefer it\n",
    "    if TRUST_CKPT_PATH_COLUMN:\n",
    "        for col in [\"ckpt\",\"ckpt_path\",\"checkpoint\",\"checkpoint_path\"]:\n",
    "            if col in df_raw.columns:\n",
    "                # try to filter row by name; else brute force across column values\n",
    "                candidates = df_raw[df_raw.get(\"name\", \"\").astype(str) == name][col].dropna().unique() if \"name\" in df_raw.columns else df_raw[col].dropna().unique()\n",
    "                for c in candidates:\n",
    "                    c = str(c)\n",
    "                    if c.endswith(f\"{name}.ckpt\") and os.path.isfile(c):\n",
    "                        return c\n",
    "    # Search filesystem\n",
    "    for root in SEARCH_ROOTS:\n",
    "        pattern = os.path.join(root, \"**\", f\"{name}.ckpt\")\n",
    "        found = glob.glob(pattern, recursive=True)\n",
    "        if found:\n",
    "            # pick most recently modified\n",
    "            found.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return found[0]\n",
    "    return None\n",
    "\n",
    "for idx, row in df_raw.iterrows():\n",
    "    name = pick_model_name(row)\n",
    "    cfg = row_to_arch_config(row)\n",
    "    if not name or not isinstance(cfg, dict):\n",
    "        continue\n",
    "    ckpt = find_ckpt_for_name(name)\n",
    "    entries.append({\"name\": name, \"arch_config\": cfg, \"ckpt_path\": ckpt})\n",
    "\n",
    "len(entries), entries[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78e68f",
   "metadata": {},
   "source": [
    "## ðŸ§· Write Sidecar `.arch.json` and Report Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "written = 0\n",
    "missing_ckpt = []\n",
    "for e in entries:\n",
    "    name, cfg, ckpt = e[\"name\"], e[\"arch_config\"], e[\"ckpt_path\"]\n",
    "    if not ckpt:\n",
    "        missing_ckpt.append(name)\n",
    "        continue\n",
    "    sidecar = ckpt.replace(\".ckpt\", \".arch.json\")\n",
    "    try:\n",
    "        with open(sidecar, \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        written += 1\n",
    "        e[\"sidecar\"] = sidecar\n",
    "    except Exception as ex:\n",
    "        print(f\"âš ï¸ Failed sidecar for {name}: {ex}\")\n",
    "\n",
    "print(f\"Sidecars written: {written}\")\n",
    "if missing_ckpt:\n",
    "    print(f\"â— {len(missing_ckpt)} models missing .ckpt file (showing up to 20):\")\n",
    "    for n in missing_ckpt[:20]:\n",
    "        print(\" -\", n)\n",
    "\n",
    "# Keep only entries that have ckpt and sidecar for validation\n",
    "entries_ready = [e for e in entries if e.get(\"ckpt_path\") and e.get(\"sidecar\")]\n",
    "print(f\"\\nEntries ready for validation: {len(entries_ready)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aba6e9",
   "metadata": {},
   "source": [
    "## ðŸ“ Metrics & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de721179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(mse(y_true, y_pred)))\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2)\n",
    "    return float(1 - ss_res / ss_tot) if ss_tot != 0 else float(\"nan\")\n",
    "\n",
    "def load_val_data(selected_collections: List[str], db_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    print(f\"ðŸ“¡ Loading validation datasets: {selected_collections}\")\n",
    "    datasets = [get_dataset(name, db_name) for name in selected_collections]\n",
    "    combined = combine_arrays(datasets)\n",
    "    shuffled = shuffle_array(combined)\n",
    "    X, y = split_combined_data(shuffled)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c573d5",
   "metadata": {},
   "source": [
    "## ðŸ§  Load Model & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_arch_and_ckpt(ckpt_path: str, arch_config: Dict[str, Any], input_size: int, output_size: int, device: torch.device) -> LightningWrapper:\n",
    "    model = GeneratedModel(input_size=input_size, output_size=output_size, architecture_config=arch_config)\n",
    "    wrapper = LightningWrapper(\n",
    "        model=model,\n",
    "        train_data=(torch.empty(1, input_size), torch.empty(1, output_size)),\n",
    "        val_data=(torch.empty(1, input_size), torch.empty(1, output_size)),\n",
    "        learning_rate=arch_config.get(\"learning_rate\", 1e-3),\n",
    "        weight_decay=arch_config.get(\"weight_decay\", 0.0),\n",
    "        optimizer_name=arch_config.get(\"optimizer\", \"adam\"),\n",
    "    )\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "    missing, unexpected = wrapper.load_state_dict(state_dict, strict=False)\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ Missing keys when loading: {missing[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "    if unexpected:\n",
    "        print(f\"âš ï¸ Unexpected keys when loading: {unexpected[:5]}{'...' if len(unexpected) > 5 else ''}\")\n",
    "    wrapper.to(device)\n",
    "    wrapper.eval()\n",
    "    return wrapper\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(wrapper: LightningWrapper, X: np.ndarray, y: np.ndarray, device: torch.device, batch_size: int = 4096):\n",
    "    X_t = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    preds = []\n",
    "    for i in range(0, X_t.size(0), batch_size):\n",
    "        xb = X_t[i:i+batch_size]\n",
    "        yb_pred = wrapper.model(xb)\n",
    "        preds.append(yb_pred.detach().cpu().numpy())\n",
    "    y_pred = np.vstack(preds)\n",
    "    metrics = {\n",
    "        \"mse\": mse(y, y_pred),\n",
    "        \"rmse\": rmse(y, y_pred),\n",
    "        \"mae\": mae(y, y_pred),\n",
    "        \"r2\": r2_score(y, y_pred),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafaab2",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subset_to_collections = {}\n",
    "for sub in SUBSETS:\n",
    "    if sub == \"collections\":\n",
    "        if not COLLECTIONS:\n",
    "            raise SystemExit('You included \"collections\" but COLLECTIONS is empty.')\n",
    "        subset_to_collections[sub] = COLLECTIONS\n",
    "    else:\n",
    "        subset_to_collections[sub] = SUBSET_MAP[sub]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "results = []\n",
    "for e in entries_ready:\n",
    "    name, ckpt = e[\"name\"], e[\"ckpt_path\"]\n",
    "    arch_config = e[\"arch_config\"]\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Model:\", name)\n",
    "    for subset_name, collections in subset_to_collections.items():\n",
    "        X_val, y_val = load_val_data(collections, DB_NAME)\n",
    "        input_size = X_val.shape[1]\n",
    "        output_size = y_val.shape[1]\n",
    "        wrapper = load_model_from_arch_and_ckpt(ckpt, arch_config, input_size, output_size, device)\n",
    "        metrics = evaluate(wrapper, X_val, y_val, device, batch_size=BATCH_SIZE)\n",
    "        row = {\n",
    "            \"name\": name,\n",
    "            \"ckpt\": ckpt,\n",
    "            \"subset\": subset_name,\n",
    "            \"collections\": \",\".join(collections),\n",
    "            **metrics\n",
    "        }\n",
    "        print(row)\n",
    "        results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values([\"name\",\"subset\"]).reset_index(drop=True)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e75c2a",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_csv = \"/mnt/data/validation_results_from_wandb.csv\"\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}