{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294c943d",
   "metadata": {},
   "source": [
    "\n",
    "# 🧪 Validate Models from W&B CSV — v2 (Robust Name/Config Parsing)\n",
    "\n",
    "This improved version:\n",
    "- Recovers **model name** from multiple CSV fields (case-insensitive) **and** from inside the parsed `config`.\n",
    "- Writes sidecar `<name>.arch.json` files next to matched checkpoints.\n",
    "- Provides **debug summaries** if something can't be matched.\n",
    "- Guards sorting and displays informative messages if no results are found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213b25a",
   "metadata": {},
   "source": [
    "## 🔧 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_PATH = \"/mnt/data/wandb_export_2025-08-22T14_23_07.979+01_00.csv\"  # your uploaded CSV\n",
    "\n",
    "# Where to search for checkpoints\n",
    "SEARCH_ROOTS = [\n",
    "    \"/home/admindi/sbenites/WirelessLocation\",\n",
    "    \"/mnt/data\",\n",
    "]\n",
    "\n",
    "# Validation subsets\n",
    "SUBSETS = [\"garage\", \"outdoor\", \"indoor\"]  # or [\"all\"] or include \"collections\"\n",
    "COLLECTIONS = []                            # used only if \"collections\" in SUBSETS\n",
    "\n",
    "DB_NAME = \"wifi_fingerprinting_data\"\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "# Project paths so imports work\n",
    "PROJECT_PATHS = [\n",
    "    \"/home/admindi/sbenites/WirelessLocation\",\n",
    "    \"/mnt/data\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3632b8",
   "metadata": {},
   "source": [
    "## 📦 Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ca816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, json, ast, math, re\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "for p in PROJECT_PATHS:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "from data_processing import get_dataset, combine_arrays, shuffle_array, split_combined_data\n",
    "from model_generation import GeneratedModel\n",
    "from gpu_fucntion import LightningWrapper\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d589a0c",
   "metadata": {},
   "source": [
    "## 🗂️ Collections & Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_COLLECTIONS = [\n",
    "    \"equilatero_grande_garage\",\n",
    "    \"equilatero_grande_outdoor\",\n",
    "    \"equilatero_medio_garage\",\n",
    "    \"equilatero_medio_outdoor\",\n",
    "    \"isosceles_grande_indoor\",\n",
    "    \"isosceles_grande_outdoor\",\n",
    "    \"isosceles_medio_outdoor\",\n",
    "    \"obtusangulo_grande_outdoor\",\n",
    "    \"obtusangulo_pequeno_outdoor\",\n",
    "    \"reto_grande_garage\",\n",
    "    \"reto_grande_indoor\",\n",
    "    \"reto_grande_outdoor\",\n",
    "    \"reto_medio_garage\",\n",
    "    \"reto_medio_outdoor\",\n",
    "    \"reto_n_quadrado_grande_indoor\",\n",
    "    \"reto_n_quadrado_grande_outdoor\",\n",
    "    \"reto_n_quadrado_pequeno_outdoor\",\n",
    "    \"reto_pequeno_garage\",\n",
    "    \"reto_pequeno_outdoor\",\n",
    "]\n",
    "\n",
    "def group_by_location(collections: List[str], locations: List[str]) -> List[str]:\n",
    "    return [name for name in collections if any(loc in name for loc in locations)]\n",
    "\n",
    "SUBSET_MAP = {\n",
    "    \"garage\": group_by_location(ALL_COLLECTIONS, [\"garage\"]),\n",
    "    \"outdoor\": group_by_location(ALL_COLLECTIONS, [\"outdoor\"]),\n",
    "    \"indoor\": group_by_location(ALL_COLLECTIONS, [\"indoor\"]),\n",
    "    \"all\": ALL_COLLECTIONS,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12744ddc",
   "metadata": {},
   "source": [
    "## 🧩 CSV → Architecture Config & Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_jsonish(text: str) -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    s = text.strip()\n",
    "    candidates = [s, s.replace(\"'\", '\"')]\n",
    "    repl = (('None','null'), ('True','true'), ('False','false'))\n",
    "    s2 = s\n",
    "    for a,b in repl:\n",
    "        s2 = re.sub(r'\\b'+a+r'\\b', b, s2)\n",
    "    candidates += [s2, s2.replace(\"'\", '\"')]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return json.loads(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            obj = ast.literal_eval(c)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def set_in_nested(d: Dict[str, Any], path_parts: List[Any], value: Any):\n",
    "    cur = d\n",
    "    for i, key in enumerate(path_parts):\n",
    "        is_last = (i == len(path_parts) - 1)\n",
    "        if isinstance(key, int):\n",
    "            if not isinstance(cur, list):\n",
    "                cur_list = []\n",
    "                if isinstance(cur, dict): cur.clear()\n",
    "                cur = cur_list\n",
    "            while len(cur) <= key:\n",
    "                cur.append({})\n",
    "            if is_last:\n",
    "                cur[key] = value\n",
    "            else:\n",
    "                if not isinstance(cur[key], (dict, list)):\n",
    "                    cur[key] = {}\n",
    "                cur = cur[key]\n",
    "        else:\n",
    "            if is_last:\n",
    "                cur[key] = value\n",
    "            else:\n",
    "                if key not in cur or not isinstance(cur[key], (dict, list)):\n",
    "                    cur[key] = {}\n",
    "                cur = cur[key]\n",
    "\n",
    "def parse_flattened_to_config(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    colnames = list(row.index)\n",
    "    prefixes = [\"config.\", \"architecture_config.\", \"arch_config.\", \"model_config.\", \"architecture.\", \"cnn_config.\"]\n",
    "    candidates = [c for c in colnames if any(c.startswith(p) for p in prefixes)]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    cfg: Dict[str, Any] = {}\n",
    "    for c in candidates:\n",
    "        val = row[c]\n",
    "        if pd.isna(val): continue\n",
    "        if isinstance(val, str):\n",
    "            if val.lower() in (\"true\",\"false\"):\n",
    "                val = (val.lower() == \"true\")\n",
    "            else:\n",
    "                try:\n",
    "                    if \".\" in val:\n",
    "                        v = float(val)\n",
    "                        val = int(v) if v.is_integer() else v\n",
    "                    else:\n",
    "                        val = int(val)\n",
    "                except Exception:\n",
    "                    parsed = parse_jsonish(val)\n",
    "                    val = parsed if parsed is not None else val\n",
    "        parts = []\n",
    "        for part in c.split(\".\"):\n",
    "            m = re.match(r\"^(.*?)(\\[(\\d+)\\])?$\", part)\n",
    "            if not m:\n",
    "                parts.append(part)\n",
    "                continue\n",
    "            name, _, idx = m.groups()\n",
    "            if name: parts.append(name)\n",
    "            if idx is not None: parts.append(int(idx))\n",
    "        if parts and parts[0] in (\"config\",\"architecture_config\",\"arch_config\",\"model_config\",\"architecture\",\"cnn_config\"):\n",
    "            parts = parts[1:]\n",
    "        if not parts: continue\n",
    "        set_in_nested(cfg, parts, val)\n",
    "    return cfg if cfg else None\n",
    "\n",
    "def row_to_arch_config(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    # direct JSON-ish\n",
    "    direct_cols = [\n",
    "        \"architecture_config\", \"arch_config\", \"config\", \"model_config\",\n",
    "        \"architecture\", \"cnn_config\", \"config_json\", \"hparams\", \"hyperparameters\",\n",
    "        \"Hyperparameters\", \"Config\", \"CONFIG\",\n",
    "    ]\n",
    "    for col in direct_cols:\n",
    "        if col in row and isinstance(row[col], str):\n",
    "            cfg = parse_jsonish(row[col])\n",
    "            if isinstance(cfg, dict): return cfg\n",
    "    # flattened\n",
    "    cfg = parse_flattened_to_config(row)\n",
    "    if isinstance(cfg, dict) and cfg: return cfg\n",
    "    return None\n",
    "\n",
    "def pick_model_name(row: pd.Series, cfg: Optional[Dict[str, Any]]) -> Optional[str]:\n",
    "    # 1) from config (most reliable for your training code)\n",
    "    if isinstance(cfg, dict):\n",
    "        for key in [\"name\", \"model_name\"]:\n",
    "            if key in cfg and isinstance(cfg[key], str) and cfg[key].strip():\n",
    "                base = os.path.basename(cfg[key].strip())\n",
    "                return os.path.splitext(base)[0]\n",
    "\n",
    "    # 2) CSV columns (case-insensitive)\n",
    "    lower_cols = {c.lower(): c for c in row.index}\n",
    "    candidates = [\"name\",\"model_name\",\"ckpt_stem\",\"run_name\",\"run name\",\"id\",\"slug\"]\n",
    "    for lc in candidates:\n",
    "        if lc in lower_cols:\n",
    "            value = row[lower_cols[lc]]\n",
    "            if isinstance(value, str) and value.strip():\n",
    "                base = os.path.basename(value.strip())\n",
    "                return os.path.splitext(base)[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bc717",
   "metadata": {},
   "source": [
    "## 📥 Load CSV & Build Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc23434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "print(\"CSV rows:\", len(df_raw))\n",
    "print(\"First 20 columns:\", list(df_raw.columns)[:20])\n",
    "\n",
    "entries = []  # {name, arch_config, ckpt_path}\n",
    "\n",
    "def find_ckpt_for_name(name: str) -> Optional[str]:\n",
    "    for root in SEARCH_ROOTS:\n",
    "        pattern = os.path.join(root, \"**\", f\"{name}.ckpt\")\n",
    "        found = glob.glob(pattern, recursive=True)\n",
    "        if found:\n",
    "            found.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return found[0]\n",
    "    return None\n",
    "\n",
    "for idx, row in df_raw.iterrows():\n",
    "    cfg = row_to_arch_config(row)\n",
    "    name = pick_model_name(row, cfg)\n",
    "    if not (isinstance(cfg, dict) and name):\n",
    "        continue\n",
    "    ckpt = find_ckpt_for_name(name)\n",
    "    entries.append({\"name\": name, \"arch_config\": cfg, \"ckpt_path\": ckpt})\n",
    "\n",
    "print(f\"Total parsed entries (config+name): {len(entries)}\")\n",
    "print(\"Preview:\", entries[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244ea23",
   "metadata": {},
   "source": [
    "## 🧷 Write Sidecars & Coverage Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1db3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "written = 0\n",
    "missing_ckpt = []\n",
    "\n",
    "for e in entries:\n",
    "    name, cfg, ckpt = e[\"name\"], e[\"arch_config\"], e[\"ckpt_path\"]\n",
    "    if not ckpt:\n",
    "        missing_ckpt.append(name); continue\n",
    "    sidecar = ckpt.replace(\".ckpt\", \".arch.json\")\n",
    "    try:\n",
    "        with open(sidecar, \"w\") as f:\n",
    "            json.dump(cfg, f)\n",
    "        written += 1\n",
    "        e[\"sidecar\"] = sidecar\n",
    "    except Exception as ex:\n",
    "        print(f\"⚠️ Sidecar failed for {name}: {ex}\")\n",
    "\n",
    "print(f\"Sidecars written: {written}\")\n",
    "if missing_ckpt:\n",
    "    print(f\"❗ {len(missing_ckpt)} models missing .ckpt (showing up to 20):\")\n",
    "    for n in missing_ckpt[:20]:\n",
    "        print(\" -\", n)\n",
    "\n",
    "entries_ready = [e for e in entries if e.get(\"ckpt_path\") and e.get(\"sidecar\")]\n",
    "print(\"Entries ready:\", len(entries_ready))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d62db5",
   "metadata": {},
   "source": [
    "## 📏 Metrics & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(mse(y_true, y_pred)))\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2)\n",
    "    return float(1 - ss_res / ss_tot) if ss_tot != 0 else float(\"nan\")\n",
    "\n",
    "def load_val_data(selected_collections: List[str], db_name: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    print(f\"📡 Loading validation datasets: {selected_collections}\")\n",
    "    datasets = [get_dataset(name, db_name) for name in selected_collections]\n",
    "    combined = combine_arrays(datasets)\n",
    "    shuffled = shuffle_array(combined)\n",
    "    X, y = split_combined_data(shuffled)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a98e0",
   "metadata": {},
   "source": [
    "## 🧠 Load Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_arch_and_ckpt(ckpt_path: str, arch_config: Dict[str, Any], input_size: int, output_size: int, device: torch.device) -> LightningWrapper:\n",
    "    model = GeneratedModel(input_size=input_size, output_size=output_size, architecture_config=arch_config)\n",
    "    wrapper = LightningWrapper(\n",
    "        model=model,\n",
    "        train_data=(torch.empty(1, input_size), torch.empty(1, output_size)),\n",
    "        val_data=(torch.empty(1, input_size), torch.empty(1, output_size)),\n",
    "        learning_rate=arch_config.get(\"learning_rate\", 1e-3),\n",
    "        weight_decay=arch_config.get(\"weight_decay\", 0.0),\n",
    "        optimizer_name=arch_config.get(\"optimizer\", \"adam\"),\n",
    "    )\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "    missing, unexpected = wrapper.load_state_dict(state_dict, strict=False)\n",
    "    if missing: print(f\"⚠️ Missing keys: {missing[:5]}{'...' if len(missing)>5 else ''}\")\n",
    "    if unexpected: print(f\"⚠️ Unexpected keys: {unexpected[:5]}{'...' if len(unexpected)>5 else ''}\")\n",
    "    wrapper.to(device); wrapper.eval()\n",
    "    return wrapper\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(wrapper: LightningWrapper, X: np.ndarray, y: np.ndarray, device: torch.device, batch_size: int = 4096):\n",
    "    X_t = torch.as_tensor(X, dtype=torch.float32, device=device)\n",
    "    preds = []\n",
    "    for i in range(0, X_t.size(0), batch_size):\n",
    "        xb = X_t[i:i+batch_size]\n",
    "        yb_pred = wrapper.model(xb)\n",
    "        preds.append(yb_pred.detach().cpu().numpy())\n",
    "    y_pred = np.vstack(preds)\n",
    "    return {\n",
    "        \"mse\": mse(y, y_pred),\n",
    "        \"rmse\": rmse(y, y_pred),\n",
    "        \"mae\": mae(y, y_pred),\n",
    "        \"r2\": r2_score(y, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1ccd4",
   "metadata": {},
   "source": [
    "## 🚀 Run Validation (with safe sorting & debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5631c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subset_to_collections = {}\n",
    "for sub in SUBSETS:\n",
    "    if sub == \"collections\":\n",
    "        if not COLLECTIONS:\n",
    "            raise SystemExit('You included \"collections\" but COLLECTIONS is empty.')\n",
    "        subset_to_collections[sub] = COLLECTIONS\n",
    "    else:\n",
    "        subset_to_collections[sub] = SUBSET_MAP[sub]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "results = []\n",
    "if not entries_ready:\n",
    "    print(\"❗ No entries are ready for validation. Check earlier cells for why (no ckpt found, no config, or no names).\")\n",
    "\n",
    "for e in entries_ready:\n",
    "    name, ckpt, cfg = e[\"name\"], e[\"ckpt_path\"], e[\"arch_config\"]\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Model:\", name)\n",
    "    for subset_name, collections in subset_to_collections.items():\n",
    "        X_val, y_val = load_val_data(collections, DB_NAME)\n",
    "        input_size = X_val.shape[1]; output_size = y_val.shape[1]\n",
    "        wrapper = load_model_from_arch_and_ckpt(ckpt, cfg, input_size, output_size, device)\n",
    "        metrics = evaluate(wrapper, X_val, y_val, device, batch_size=BATCH_SIZE)\n",
    "        row = {\"name\": name, \"ckpt\": ckpt, \"subset\": subset_name, \"collections\": \",\".join(collections), **metrics}\n",
    "        print(row)\n",
    "        results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "if not df_results.empty:\n",
    "    cols_to_sort = [c for c in [\"name\",\"subset\"] if c in df_results.columns]\n",
    "    if cols_to_sort:\n",
    "        df_results = df_results.sort_values(cols_to_sort).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"ℹ️ No results produced — likely zero matched (name → ckpt) or parsing failed.\")\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6d5f0",
   "metadata": {},
   "source": [
    "## 💾 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not df_results.empty:\n",
    "    out_csv = \"/mnt/data/validation_results_from_wandb_v2.csv\"\n",
    "    df_results.to_csv(out_csv, index=False)\n",
    "    print(\"Saved:\", out_csv)\n",
    "else:\n",
    "    print(\"No results to save.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}